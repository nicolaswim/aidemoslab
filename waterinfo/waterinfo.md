
# Water usage of AI
*Nicolas Wim Landuyt*

## Introduction

The demand for AI systems has risen rapidly over the last decade.
Companies are investing massive amounts into being able to house these
humongous systems [@isaev_data_2023]. The training and maintenance
(inference is the official term) is done through the use of data
centers. These data centers have started scaling up in size and power
consumption, and thus also in water and electricity usage. Within the
scope of this project, we will be considering the water usage of these
systems.

## Scope

The idea to focus on sustainability within these ethical frameworks
stems from their current lack of attention
[@van_wynsberghe_sustainable_2021]. However, recently more research has
been conducted on the ecological impact of AI, exploring how to make AI
more sustainable and address its own environmental impact. Not only has
research begun using AI to improve the sustainability of the world
(using satellite images for better forecasting, estimating the effects
of certain emissions, etc.), but it has also shifted towards making AI
itself more sustainable.

Within the broad scope of sustainability, I have narrowed it down to the
use of water in AI systems. This focus arises due to the imbalance in
the lack of research, measurements, and policies surrounding water usage
in AI systems and their immense water consumption. Like energy usage,
water is one of the pillars underpinning the AI systems we see around us
[@george_environmental_2023], yet it is almost never considered as a
parameter to optimize. Most current efforts within the AI/ML community
are focused on the energy consumption and carbon footprint of developing
and operating AI systems, leaving the usage of the most valuable
resource on the planet undocumented.

The decision also concerns water usage equity. "it is estimated that 20%
of datacenter servers' direct water footprint is sourced from moderately
to highly stressed watersheds and 50% of servers are at least partially
supplied by power plants in water stressed areas
[@siddik_environmental_2021]." [@zhao_greener_2022]. This highlights the
growing problem of water demand, where the water used is sourced from
stressed areas. This could potentially lead to a resource injustice
where water-rich regions consume more AI systems, while water-poor
regions bear the cost.

## Water Cooling

Traditional HVAC systems (air-controlled cooling systems) are very
costly to maintain. So the industry has turned to water cooling systems.
These systems are more efficient and cost-effective
[@noauthor_liquid_nodate]. Many different types of water cooling are
used in these data centers (e.g., direct water cooling, indirect water
cooling, etc.). All of these systems, however, use the evaporation of
water to dissipate energy and cool the systems. This is the direct usage
of water, however, it is not the only source of water usage. Many of
these data centers require enormous amounts of energy in the form of
electricity. This electricity is generated by power plants. These power
plants require water to cool the systems. This is the indirect usage of
water [@ristic_water_2015].

<img src="https://github.com/nicolaswim/aidemoslab/raw/main/images/dc_evaporative-cooling-system_0.jpg" width="85%">


The researchers at *UC Riverside* and *UT Arlington* have highlighted
the significant water footprint of AI models, emphasizing the urgent
need to address freshwater scarcity and underscored the importance of
considering both water and carbon footprints for achieving sustainable
AI development [@li_making_2023]. They found that for every 20-50
queries (interactions you have with chatGPT) there is an estimated of
500ml water used (1 drinking bottle of water). These findings are
interesting, not only because they expose the scale of water usage and
raise questions like how do these systems use water? Cooling water gets
recycled, right? These are all questions I found myself and my peers
asking and then looking up what was actually going on \"under the hood\"
of these cooling systems exposed the reality of the water evaporation
techniques. It is an interesting finding because it shows that AI
systems do actually require a lot resources to preform inference.

In the development of AI systems, training and developing the model
trump the inference cost. This means developing a model requires much
more energy and water than running it. The architectures have been
built, the neural nets, their weights and biases have all been learned
and assigned, and the hyperparameter tuning has finished; running a
model is then just a question of multiple simple calculations, running
the input through the systems, and finally acquiring a result at a
relatively low cost (in terms of computing power). However, more and
more research has started making the case that the widespread use of AI
has increased substantially so that the inference costs have not only
increased so much that they shouldn't be neglected anymore
[@hessenthaler_bridging_2022], but also \"they account for a large
proportion of the data center compute cycles\" [@li_clover_2023].

\"For example, many of Google's billion-user services are empowered by
AI and their inference represents 60% of the AI infrastructure emissions
[@patterson_carbon_2022]; Meta has expanded their infrastructure
capacity by 2.5Ã— to meet the ML inference demand [@wu_sustainable_2022];
AWS and NVIDIA have estimated that inference accounts for 90% of the ML
workloads in HPC and cloud datacenters
[@noauthor_amazon_2019][@leopold_aws_2019].\"[@li_clover_2023]

## Complications

The complexity of addressing water usage in AI and data centers is
compounded by the lack of transparency and data. Currently, companies
aren't required to disclose their water usage [@ristic_water_2015]. Many
data centers don't even track their water consumption. Despite various
companies committing to water sustainability goals, the absence of
robust measures and reporting frameworks undermines these efforts. This
lack of data infrastructure hinders the verification of water usage
claims, leading to an increased focus on energy consumption and carbon
footprints, which are easier to measure by simply consulting with grid
operators about specific energy demands.

## Conclusion

Sustainability in AI engineering needs to look beyond mere environmental
impacts like carbon footprints and consider the broader ethical, social,
and technical landscapes. Simply focusing on carbon numbers might miss
the larger ethical questions and the intrinsic value of maintaining a
sustainable environment [@richie_environmentally_2022]. It's crucial
that AI systems are evaluated not in isolation but as part of their
wider ecological and socio-technical contexts, which highlights the need
to understand the interconnectedness of technology and society
[@bolte_conceptual_2023]. Using a holistic approach with indicators can
help in the practical implementation of sustainable AI, ensuring it not
only meets environmental standards but also fairly distributes benefits
and risks across society [@rohde_broadening_2024]. This perspective
shows the importance of moving from isolated technical fixes to broader
societal changes.